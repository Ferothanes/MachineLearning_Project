{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0434b960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of transcripts: 53\n"
     ]
    }
   ],
   "source": [
    "from backend.constants import DATA_PATH\n",
    "\n",
    "# List all Markdown transcript files\n",
    "transcripts = list(DATA_PATH.glob(\"*.md\"))\n",
    "\n",
    "# Print the number of transcripts\n",
    "print(f\"Number of transcripts: {len(transcripts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a624298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# An introduction to the vector database LanceDB\n",
      "\n",
      "[00:00:00] Hello and welcome to this video where we'll go into Lance db, the fundamentals of it. Lance Db is an open source vector database designed to handle large scale ~~data, ~~vector data efficiently. It provides a robust platform for storing, indexing and querying high dimensional vector. Which is very good for working with for example, LLMs and rag applications.\n",
      "\n",
      "Rags are retrieval, augmented generation. ~~When, ~~whenever you want to chat with your data or chat with your documentation you are providing a query, ~~right? ~~A prompt or a query, a text which is then transformed into vectors. And this vector is compared. To all the other chunks or all the other documents in your database.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preview first 5 lines of the first transcript\n",
    "first_file = list(DATA_PATH.glob(\"*.md\"))[0]\n",
    "with open(first_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "print(\"\".join(lines[:5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fe4f2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty transcripts: []\n"
     ]
    }
   ],
   "source": [
    "# Check for empty transcripts\n",
    "empty_files = [f.name for f in DATA_PATH.glob(\"*.md\") if f.stat().st_size == 0]\n",
    "print(f\"Empty transcripts: {empty_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "608a903e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An introduction to the vector database LanceDB.md: 6119 words\n",
      "API trafiklab (1).md: 5298 words\n",
      "API trafiklab.md: 5298 words\n",
      "Azure static web app deploy react app.md: 1949 words\n",
      "Chat with your excel data - xlwings lite (1).md: 4361 words\n",
      "Chat with your excel data - xlwings lite.md: 4361 words\n",
      "Course structure for Azure two weeks course.md: 1567 words\n",
      "Data platform course structure.md: 1548 words\n",
      "data processing course  structure.md: 1655 words\n",
      "data storytelling.md: 2353 words\n",
      "dbt modeling snowflake.md: 7088 words\n",
      "docker setup windows.md: 1900 words\n",
      "FastAPI and scikit-learn API connect to streamlit frontend.md: 2701 words\n",
      "Fastapi CRUD app.md: 4509 words\n",
      "Hands on regularization.md: 4260 words\n",
      "How does LLM work_.md: 2509 words\n",
      "Logistic regression hands on with scikit learn.md: 3212 words\n",
      "Logistic regression theory.md: 2226 words\n",
      "Modern data stack - deploy dockerized dashboard into Azure web app.md: 2286 words\n",
      "Modern data stack - dockerize your data pipeline.md: 3415 words\n",
      "Modern data stack - using dlt to extract and load api data to snowflake.md: 5198 words\n",
      "Packaging in python.md: 4410 words\n",
      "pandas_read_excel.md: 4610 words\n",
      "postgres sink.md: 5751 words\n",
      "Pydantic fundamentals.md: 4734 words\n",
      "Pydantic with gemini to structure output in a much neater way.md: 2454 words\n",
      "pydanticAI chatbot.md: 3617 words\n",
      "PydanticAI fundamentals - outputting structured pydantic model.md: 2984 words\n",
      "pytest unit testing.md: 2383 words\n",
      "Python fundamentals.md: 7341 words\n",
      "python intro.md: 1011 words\n",
      "Python_oop_1.md: 2990 words\n",
      "Serving PydanticAI Gemini Model with FastAPI Tutorial (1).md: 2859 words\n",
      "Serving PydanticAI Gemini Model with FastAPI Tutorial.md: 2859 words\n",
      "SQL analytics course with DuckDB - course structure.md: 2211 words\n",
      "SQL analytics course with DuckDB - CRUD operations tutorial.md: 5288 words\n",
      "SQL analytics course with DuckDB - dlt to load sakila data from SQLite into DuckDB.md: 2554 words\n",
      "SQL analytics course with DuckDB - joins concepts.md: 1561 words\n",
      "SQL analytics course with DuckDB - joins with sakila database tutorial.md: 3391 words\n",
      "SQL analytics course with DuckDB - pandas and duckdb (1).md: 4777 words\n",
      "SQL analytics course with DuckDB - pandas and duckdb.md: 4777 words\n",
      "SQL analytics course with DuckDB - Sakila BI dashboard using Evidence (1).md: 4659 words\n",
      "SQL analytics course with DuckDB - Sakila BI dashboard using Evidence.md: 4659 words\n",
      "SQL analytics course with DuckDB - set theory part 2 (using Sakila) (1).md: 2938 words\n",
      "SQL analytics course with DuckDB - set theory part 2 (using Sakila).md: 2938 words\n",
      "SQL analytics course with DuckDB - setup duckdb.md: 3207 words\n",
      "SQL analytics course with DuckDB - strings concepts.md: 1302 words\n",
      "SQL analytics course with DuckDB - strings tutorial.md: 4605 words\n",
      "SQL analytics course with DuckDB - subquery tutorial.md: 1884 words\n",
      "SQL analytics course with DuckDB - views tutorial.md: 2159 words\n",
      "SQL analytics with DuckDB - introduction.md: 2196 words\n",
      "Terraform setup.md: 3345 words\n",
      "XGBoost hands on tutorial for classification.md: 2339 words\n"
     ]
    }
   ],
   "source": [
    "# Compute number of words per transcript\n",
    "for file in DATA_PATH.glob(\"*.md\"):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    print(f\"{file.name}: {len(content.split())} words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bceeeea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404 \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "url = f\"https://transcriptchatbot.azurewebsites.net/api/function_app/rag/query?code={os.getenv('AZURE_FUNCTION_KEY')}\"\n",
    "response = requests.post(url, json={\"prompt\": \"test\"})\n",
    "print(response.status_code, response.text)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
